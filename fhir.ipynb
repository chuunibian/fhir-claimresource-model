{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEF+h3BbRvmrXSy4fjNTY0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chuunibian/fhir-claimresource-model/blob/main/fhir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So SyntheaPatientModel will take in the json and then use the parse to extract information, it is flexible what to extract but first try to get all of the claims."
      ],
      "metadata": {
        "id": "2kPU1HswgVe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Macros\n",
        "'''\n",
        "MAX_CLAIM_C = 30\n",
        "BATCH_SIZE_C = 8\n",
        "CLAIM_ENCODE_SIZE = 22"
      ],
      "metadata": {
        "id": "UGPdvqq3XBUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "fhir_test_data = \"/content/drive/MyDrive/synthea_sample_data_fhir_latest\"\n",
        "data_path = Path(fhir_test_data)\n",
        "\n",
        "files = os.listdir(data_path)\n",
        "\n",
        "if files:\n",
        "  file = data_path / Path(files[5])  # get first file\n",
        "  print(file)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iLNyEN9j8xh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b0f36b1-f79b-46be-8418-d26793e2cec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/synthea_sample_data_fhir_latest/Caryl47_Kassulke119_4569671e-ed39-055f-8e78-422b96c9896b.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Validation Which will be done at begenning\n",
        "try:\n",
        "  import fhir.resources\n",
        "except ImportError:\n",
        "  !pip install fhir.resources\n",
        "\n",
        "from fhir.resources.R4B.bundle import Bundle\n",
        "\n",
        "def validate_fhir_bundle(json_string):\n",
        "    try:\n",
        "        bundle = Bundle.model_validate_json(json_string)\n",
        "        return True, \"Valid FHIR Bundle\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Invalid FHIR Bundle: {str(e)}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_qscyMIu-fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "class Claim:\n",
        "  '''\n",
        "  What goes into claim_instance is a dict rep of a claim resource\n",
        "\n",
        "  each of claim object represents one claim resource\n",
        "  '''\n",
        "  def __init__(self, claim_instance):\n",
        "        self.claim_id = self._extract_claim_id(claim_instance)\n",
        "\n",
        "        self.status = self._extract_status(claim_instance)\n",
        "\n",
        "        self.type_of_claim = self._extract_type_of_claim(claim_instance)\n",
        "\n",
        "        self.type_of_subclaim = self._extract_type_of_subclaim(claim_instance)\n",
        "\n",
        "        self.bill_period = self._extract_bill_period(claim_instance)\n",
        "\n",
        "        self.claim_creation = self._extract_claim_creation(claim_instance)\n",
        "\n",
        "        self.priority = self._extract_priority(claim_instance)\n",
        "\n",
        "        self.total = self._extract_total(claim_instance)\n",
        "\n",
        "        self.number_of_diagnoses = self._extract_number_of_diagnoses(claim_instance)\n",
        "\n",
        "        self.number_of_items = self._extract_number_of_items(claim_instance)\n",
        "\n",
        "        self.number_of_drugs = self._extract_number_of_items(claim_instance)  # assuming pharmacy claims\n",
        "\n",
        "        self.billable_duration_days = self._calculate_billable_duration_days()\n",
        "\n",
        "        self.insurance = self._extract_insurance(claim_instance)\n",
        "\n",
        "  def _extract_claim_id(self, claim_instance):\n",
        "        if 'id' in claim_instance:\n",
        "            return claim_instance['id']\n",
        "        return None\n",
        "\n",
        "  def _extract_insurance(self, claim_instance):\n",
        "        insurance_entries = claim_instance.get(\"insurance\", [])\n",
        "        return len(insurance_entries)\n",
        "\n",
        "  def _extract_status(self, claim_instance):\n",
        "        if 'status' in claim_instance:\n",
        "            return claim_instance['status']\n",
        "        return None\n",
        "\n",
        "  def _extract_type_of_claim(self, claim_instance):\n",
        "        if 'type' in claim_instance:\n",
        "            type_obj = claim_instance['type']\n",
        "            if 'coding' in type_obj and type_obj['coding'] and 'display' in type_obj['coding'][0]:\n",
        "                return type_obj['coding'][0]['display']\n",
        "            elif 'coding' in type_obj and type_obj['coding'] and 'code' in type_obj['coding'][0]:\n",
        "                return type_obj['coding'][0]['code']\n",
        "        return None\n",
        "\n",
        "  def _extract_type_of_subclaim(self, claim_instance):\n",
        "        if 'subType' in claim_instance:\n",
        "            subtype_obj = claim_instance['subType']\n",
        "            if 'coding' in subtype_obj and subtype_obj['coding'] and 'display' in subtype_obj['coding'][0]:\n",
        "                return subtype_obj['coding'][0]['display']\n",
        "            elif 'coding' in subtype_obj and subtype_obj['coding'] and 'code' in subtype_obj['coding'][0]:\n",
        "                return subtype_obj['coding'][0]['code']\n",
        "        return None\n",
        "\n",
        "  def _extract_bill_period(self, claim_instance):\n",
        "    if 'billablePeriod' in claim_instance:\n",
        "        period = claim_instance['billablePeriod']\n",
        "        start = period.get('start')\n",
        "        end = period.get('end')\n",
        "\n",
        "        def convert_to_numeric(date_str):\n",
        "            dt = datetime.fromisoformat(date_str)\n",
        "            return int(dt.strftime('%Y%m%d%H%M%S'))\n",
        "\n",
        "        if start and end:\n",
        "            return {\n",
        "                'start': convert_to_numeric(start),\n",
        "                'end': convert_to_numeric(end)\n",
        "            }\n",
        "        elif start:\n",
        "            return {'start': convert_to_numeric(start)}\n",
        "        elif end:\n",
        "            return {'end': convert_to_numeric(end)}\n",
        "\n",
        "    return None\n",
        "\n",
        "  def _extract_claim_creation(self, claim_instance):\n",
        "        if 'created' in claim_instance:\n",
        "            return claim_instance['created']\n",
        "        return None\n",
        "\n",
        "  def _extract_priority(self, claim_instance):\n",
        "        if 'priority' in claim_instance:\n",
        "            priority_obj = claim_instance['priority']\n",
        "            if 'coding' in priority_obj and priority_obj['coding'] and 'display' in priority_obj['coding'][0]:\n",
        "                return priority_obj['coding'][0]['display']\n",
        "            elif 'coding' in priority_obj and priority_obj['coding'] and 'code' in priority_obj['coding'][0]:\n",
        "                return priority_obj['coding'][0]['code']\n",
        "        return None\n",
        "\n",
        "  def _extract_total(self, claim_instance):\n",
        "        if 'total' in claim_instance:\n",
        "            total_obj = claim_instance['total']\n",
        "            value = total_obj.get('value')\n",
        "            currency = total_obj.get('currency')\n",
        "\n",
        "            if value is not None:\n",
        "                if currency:\n",
        "                    return {'value': value, 'currency': currency}\n",
        "                return {'value': value}\n",
        "        return None\n",
        "\n",
        "  # TODO THIS IS WRONG???\n",
        "  def _extract_number_of_diagnoses(self, claim_instance):\n",
        "        diagnoses = claim_instance.get(\"diagnosis\", [])\n",
        "        return len(diagnoses)\n",
        "\n",
        "\n",
        "  def _extract_number_of_items(self, claim_instance):\n",
        "        items = claim_instance.get('item', [])\n",
        "        return len(items)\n",
        "\n",
        "  def _calculate_billable_duration_days(self):\n",
        "        start = self.bill_period.get('start')\n",
        "        end = self.bill_period.get('end')\n",
        "        if start and end:\n",
        "            try:\n",
        "                start_dt = datetime.fromisoformat(start.replace('Z', '+00:00'))\n",
        "                end_dt = datetime.fromisoformat(end.replace('Z', '+00:00'))\n",
        "                duration = (end_dt - start_dt).days\n",
        "                return max(duration, 0)\n",
        "            except Exception:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "  def __repr__(self):\n",
        "        # Custom representation of the Claim object\n",
        "        return f\"Claim(claim_id={self.claim_id})\"\n",
        "\n"
      ],
      "metadata": {
        "id": "IkeLS_5QhOXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If using instance var you need self and you need to pass in self to function so that the funciton knows what object to call on"
      ],
      "metadata": {
        "id": "E7XaTcBv82p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "\n",
        "class SyntheaGenerationFHIRCustomParser:\n",
        "  \"\"\"\n",
        "  Custom parser to extract resources and data within those resources\n",
        "  of R4 FHIR .json data.\n",
        "\n",
        "  For instance of class it will generate a custom object represeting parsed\n",
        "  values from the FHIR .json\n",
        "\n",
        "  Main job is to find [X_type] resource and then pass each instance of found\n",
        "  resource into constructor for wrapper class then return a list of those wrapper classes\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, json_path: str):\n",
        "    self.file_path = json_path\n",
        "    self.parsed_json = {}\n",
        "\n",
        "    with open(json_path, 'r') as file:\n",
        "      self.parsed_json = json.load(file)\n",
        "\n",
        "\n",
        "\n",
        "  def get_list_of_claims(self) -> Dict[int, Claim]:\n",
        "\n",
        "    list_of_claim = {}\n",
        "    counter = 0\n",
        "\n",
        "    # Get all the claim entries\n",
        "    if 'entry' in self.parsed_json:\n",
        "      entries = self.parsed_json['entry']\n",
        "      for entry in entries:\n",
        "        if 'resource' in entry:\n",
        "          resource = entry['resource'] # Gets a resource from entries\n",
        "          if 'Claim' in resource.get('resourceType'):\n",
        "            # print(f\"Found claim {counter}\")\n",
        "            temp_claim = Claim(resource)\n",
        "            list_of_claim[counter] = temp_claim # insert index to claim pair\n",
        "            counter+=1\n",
        "\n",
        "\n",
        "    return list_of_claim\n",
        "\n",
        "  def get_list_of_medications():\n",
        "      pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p2OY33A-cRrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Union\n",
        "from collections import defaultdict\n",
        "\n",
        "class SyntheaBundleModel:\n",
        "    \"\"\"\n",
        "    A flexible model for representing Synthea patient data with varying structures\n",
        "    used for Synthea FHIR R4 data generations\n",
        "\n",
        "    The model will have representations of the various different resources within\n",
        "    FHIR Standard\n",
        "\n",
        "    It will go like list of [X Type] resource contained within this object\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, json_path: str):\n",
        "      parser = SyntheaGenerationFHIRCustomParser(json_path)\n",
        "      self.list_of_claim = parser.get_list_of_claims()\n",
        "      #print(len(self.list_of_claim))\n",
        "\n"
      ],
      "metadata": {
        "id": "uYKIztVgfB0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fhir_json_instance = SyntheaBundleModel(file)\n",
        "\n",
        "print(len(fhir_json_instance.list_of_claim))\n",
        "\n",
        "print(fhir_json_instance.list_of_claim[1].claim_id) # from claim obj get such attrib\n",
        "\n",
        "print(fhir_json_instance.list_of_claim[3].type_of_claim)\n",
        "\n",
        "print(fhir_json_instance.list_of_claim[3].number_of_diagnoses)\n",
        "\n",
        "print(fhir_json_instance.list_of_claim[3].bill_period)\n",
        "\n",
        "print(fhir_json_instance.list_of_claim[3].billable_duration_days)\n",
        "\n",
        "print(fhir_json_instance.list_of_claim[3].number_of_items)\n",
        "\n",
        "print(fhir_json_instance.list_of_claim[3].number_of_diagnoses)\n",
        "\n",
        "print(fhir_json_instance.list_of_claim[3].priority)\n",
        "\n",
        "print(fhir_json_instance.list_of_claim[3].status)\n",
        "\n",
        "print(fhir_json_instance.list_of_claim[3].insurance)\n",
        "\n",
        "print(fhir_json_instance.list_of_claim[3].total)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iBkavlRC7Ix1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a036acbf-b180-4625-f05c-52db9481c251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n",
            "2b85d8e7-a230-5a3e-f47b-b6bf1c8d2f9b\n",
            "pharmacy\n",
            "0\n",
            "{'start': 20140726142141, 'end': 20140726144950}\n",
            "None\n",
            "1\n",
            "0\n",
            "normal\n",
            "active\n",
            "1\n",
            "{'value': 299.3, 'currency': 'USD'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "Python is essentially doing the following under the hood:\n",
        "\n",
        "It creates a function object for each method (such as __init__ and say_hello).\n",
        "These function objects are stored in the __dict__ of the class object (MyClass).\n",
        "When an instance is created from the class, these function objects are linked to the instance through the class object. The instance doesn't directly store the function; instead, it looks up the function in the class object."
      ],
      "metadata": {
        "id": "xoDxra3q-7XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "First maybe try aggregated single vector\n",
        "\n",
        "then try X max amount of claims probably most recent ones or if not enough to meet\n",
        "X then take all exisitng and then just do padding then input to autoencoder will be\n",
        "[Max claim X static length of each claim]\n",
        "'''"
      ],
      "metadata": {
        "id": "racQXb-59UJY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3f0ad0d0-8b2a-415c-cc26-a06d350d3871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFirst maybe try aggregated single vector\\n\\nthen try X max amount of claims probably most recent ones or if not enough to meet\\nX then take all exisitng and then just do padding then input to autoencoder will be\\n[Max claim X static length of each claim]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "OneHotEnc_claim_types = ['institutional', 'professional', 'oral', 'pharmacy', 'vision', 'hearing', 'others']\n",
        "OneHotEnc_status_types = ['active', 'cancelled', 'rejected', 'pending', 'completed', 'others']\n",
        "OneHotEnc_priority_types = ['normal', 'stat', 'deferred', 'others']\n",
        "BinaryFlag_insuranceCoverage = [0,1] # 0 is no 1 is yes\n",
        "\n",
        "'''\n",
        "returns a static lengthed vector encoding for the passed in claim\n",
        "'''\n",
        "def encode_claim(claim: Claim) -> np.ndarray:\n",
        "\n",
        "  claim_vector = np.array([])\n",
        "\n",
        "  # print(claim)\n",
        "\n",
        "  # Concat billable period start 19960210094418 L(1)\n",
        "  # claim_vector = np.append(claim_vector, claim.bill_period['start'])\n",
        "\n",
        "  # Concat billable period end L(1)\n",
        "  # claim_vector = np.append(claim_vector, claim.bill_period['end'])\n",
        "\n",
        "  # Concat type ONE HOT ENC L(7)\n",
        "  type_temp = [0] * len(OneHotEnc_claim_types)\n",
        "  if claim.type_of_claim in OneHotEnc_claim_types:\n",
        "    type_temp[OneHotEnc_claim_types.index(claim.type_of_claim)] = 1\n",
        "  else:\n",
        "    type_temp[OneHotEnc_claim_types.index('others')] = 1\n",
        "\n",
        "  claim_vector = np.append(claim_vector, type_temp)\n",
        "\n",
        "  # Concat status ONE HOT ENC L(6)\n",
        "  type_status = [0] * len(OneHotEnc_status_types)\n",
        "  if claim.status in OneHotEnc_status_types:\n",
        "    type_status[OneHotEnc_status_types.index(claim.status)] = 1\n",
        "  else:\n",
        "    type_status[OneHotEnc_status_types.index('others')] = 1\n",
        "\n",
        "  claim_vector = np.append(claim_vector, type_status)\n",
        "\n",
        "  # Concat priority ONE HOT ENC L(4)\n",
        "  priority_status = [0] * len(OneHotEnc_priority_types)\n",
        "  if claim.priority in OneHotEnc_priority_types:\n",
        "    priority_status[OneHotEnc_priority_types.index(claim.priority)] = 1\n",
        "  else:\n",
        "    priority_status[OneHotEnc_priority_types.index('others')] = 1\n",
        "\n",
        "  claim_vector = np.append(claim_vector, priority_status)\n",
        "\n",
        "  # Concat insurance L(1)\n",
        "  if claim.insurance is not None:\n",
        "    claim_vector = np.append(claim_vector, claim.insurance)\n",
        "\n",
        "  # Concat number of drugs L(1)\n",
        "  claim_vector = np.append(claim_vector, claim.number_of_drugs)\n",
        "\n",
        "  # Concat number of items L(1)\n",
        "  claim_vector = np.append(claim_vector, claim.number_of_items)\n",
        "\n",
        "  # Concat number of diagnoises L(1)\n",
        "  claim_vector = np.append(claim_vector, claim.number_of_diagnoses)\n",
        "\n",
        "  # Concat total costs L(1)\n",
        "  # TODO: maybe need normalization based on type of currency currently only assume usd\n",
        "  # claim_vector = np.append(claim_vector, claim.total['value'])\n",
        "  claim_vector = np.append(claim_vector, 1)\n",
        "\n",
        "\n",
        "  return claim_vector\n",
        "\n",
        "\n",
        "'''\n",
        "Create the model input\n",
        "'''\n",
        "def create_model_input(list_of_claim: List, max_claim: int, claim_vector_size: int) -> torch.Tensor:\n",
        "\n",
        "  np_model_input = np.empty((max_claim, claim_vector_size))\n",
        "\n",
        "  if len(list_of_claim) < max_claim:\n",
        "    number_of_claims = len(list_of_claim)\n",
        "    print(f\"Padded person claim count {number_of_claims}\")\n",
        "    for idx in range(number_of_claims):\n",
        "      temp_vector = encode_claim(list_of_claim[idx])\n",
        "      np_model_input[idx,:] = temp_vector\n",
        "    # Rest is padding zeros\n",
        "    for idx in range(number_of_claims, max_claim):\n",
        "      np_model_input[idx,:] = np.zeros(claim_vector_size)\n",
        "\n",
        "    # TODO MAYBE NEED TO MAKE CHANGES TO THIS\n",
        "\n",
        "  else:\n",
        "    # Take the most recent max_claim claims\n",
        "    count = 0\n",
        "    for idx in range(len(list_of_claim) - max_claim, len(list_of_claim)):\n",
        "      temp_vector = encode_claim(list_of_claim[idx])\n",
        "      np_model_input[count,:] = temp_vector\n",
        "      count+=1\n",
        "\n",
        "  return torch.from_numpy(np_model_input).float()\n"
      ],
      "metadata": {
        "id": "_iIsqlcQ2_db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Second attempt at encode claim\n",
        "this time making it a multi dimensional tensor\n",
        "'''\n",
        "\n",
        "def encode_claim2(claim: Claim) -> np.ndarray:\n",
        "\n",
        "  claim_vector = np.array([])\n",
        "\n",
        "  print(claim)\n",
        "\n",
        "  # Concat billable period start 19960210094418 L(1)\n",
        "  claim_vector = np.append(claim_vector, claim.bill_period['start'])\n",
        "\n",
        "  # Concat billable period end L(1)\n",
        "  claim_vector = np.append(claim_vector, claim.bill_period['end'])\n",
        "\n",
        "  # Concat type ONE HOT ENC L(7)\n",
        "  type_temp = [0] * len(OneHotEnc_claim_types)\n",
        "  if claim.type_of_claim in OneHotEnc_claim_types:\n",
        "    type_temp[OneHotEnc_claim_types.index(claim.type_of_claim)] = 1\n",
        "  else:\n",
        "    type_temp[OneHotEnc_claim_types.index('others')] = 1\n",
        "\n",
        "  claim_vector = np.append(claim_vector, type_temp)\n",
        "\n",
        "  # Concat status ONE HOT ENC L(6)\n",
        "  type_status = [0] * len(OneHotEnc_status_types)\n",
        "  if claim.status in OneHotEnc_status_types:\n",
        "    type_status[OneHotEnc_status_types.index(claim.status)] = 1\n",
        "  else:\n",
        "    type_status[OneHotEnc_status_types.index('others')] = 1\n",
        "\n",
        "  claim_vector = np.append(claim_vector, type_status)\n",
        "\n",
        "  # Concat priority ONE HOT ENC L(4)\n",
        "  priority_status = [0] * len(OneHotEnc_priority_types)\n",
        "  if claim.priority in OneHotEnc_priority_types:\n",
        "    priority_status[OneHotEnc_priority_types.index(claim.priority)] = 1\n",
        "  else:\n",
        "    priority_status[OneHotEnc_priority_types.index('others')] = 1\n",
        "\n",
        "  claim_vector = np.append(claim_vector, priority_status)\n",
        "\n",
        "  # Concat insurance L(1)\n",
        "  if claim.insurance is not None:\n",
        "    claim_vector = np.append(claim_vector, claim.insurance)\n",
        "\n",
        "  # Concat number of drugs L(1)\n",
        "  claim_vector = np.append(claim_vector, claim.number_of_drugs)\n",
        "\n",
        "  # Concat number of items L(1)\n",
        "  claim_vector = np.append(claim_vector, claim.number_of_items)\n",
        "\n",
        "  # Concat number of diagnoises L(1)\n",
        "  claim_vector = np.append(claim_vector, claim.number_of_diagnoses)\n",
        "\n",
        "  # Concat total costs L(1)\n",
        "  # TODO: maybe need normalization based on type of currency currently only assume usd\n",
        "  claim_vector = np.append(claim_vector, claim.total['value'])\n",
        "\n",
        "  return claim_vector"
      ],
      "metadata": {
        "id": "zE3ykU1R19xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "^ For that above possibly add warning if too less claims and too much padding\n",
        "\n",
        "\n",
        "Can also try masking\n",
        "\n",
        "normalization based on number of claims also another option\n",
        "\n",
        "input_tensor: Your padded claims data (shape: [max_claim, claim_vector_size])\n",
        "mask: A separate vector indicating which positions contain real data (shape: [max_claim])\n",
        "\n"
      ],
      "metadata": {
        "id": "3Kj2lqLnkFWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Testing the whole thign out\n",
        "'''\n",
        "\n",
        "fhir_json_instance = SyntheaBundleModel(file)\n",
        "\n",
        "tensor = create_model_input(fhir_json_instance.list_of_claim, MAX_CLAIM_C, CLAIM_ENCODE_SIZE)\n",
        "print(tensor.dtype)\n",
        "print(tensor.shape)"
      ],
      "metadata": {
        "id": "MnbnuORfus5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a566d5b9-8ed4-428e-b55c-75a3778a2311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "torch.Size([30, 22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Model\n",
        "\n",
        "Need to study it internally\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMAutoEncoder(nn.Module):\n",
        "  def __init__(self, input_dim: int, hidden_dim: int, latent_dim: int, num_layers: int):\n",
        "    super().__init__()\n",
        "    self.encoder = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    self.latent_fc = nn.Linear(hidden_dim, latent_dim)\n",
        "    self.decoder_fc = nn.Linear(latent_dim, hidden_dim)\n",
        "\n",
        "    self.decoder = nn.LSTM(input_size=hidden_dim, hidden_size=input_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Encode\n",
        "    _, (hidden_state_temp, _) = self.encoder(x)\n",
        "    latent_vector = self.latent_fc(hidden_state_temp[-1])  # Take last layer’s hidden state, map to latent dim\n",
        "\n",
        "    # Decode\n",
        "    hidden_state = self.decoder_fc(latent_vector).unsqueeze(0)  # Expand back to hidden_dim\n",
        "    output, _ = self.decoder(hidden_state.repeat(x.size(1), 1, 1).permute(1, 0, 2))  # Reconstruct sequence\n",
        "\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "ip3GDJF4YhcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Test out model shapes\n",
        "'''\n",
        "\n",
        "try:\n",
        "  import torchinfo\n",
        "except:\n",
        "  !pip install torchinfo\n",
        "  import torchinfo\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "model_0 = LSTMAutoEncoder(CLAIM_ENCODE_SIZE, 64,16, 2)\n",
        "summary(model_0, input_size = [8, 30, 22])\n",
        "\n",
        "# print(list(model_0.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNXG22akDTg6",
        "outputId": "51543b01-17db-4195-8a9f-d0ab12822acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "LSTMAutoEncoder                          [8, 30, 22]               --\n",
              "├─LSTM: 1-1                              [8, 30, 128]              209,920\n",
              "├─Linear: 1-2                            [8, 32]                   4,128\n",
              "├─Linear: 1-3                            [8, 128]                  4,224\n",
              "├─LSTM: 1-4                              [8, 30, 22]               17,424\n",
              "==========================================================================================\n",
              "Total params: 235,696\n",
              "Trainable params: 235,696\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 54.63\n",
              "==========================================================================================\n",
              "Input size (MB): 0.02\n",
              "Forward/backward pass size (MB): 0.30\n",
              "Params size (MB): 0.94\n",
              "Estimated Total Size (MB): 1.26\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Custom dataset\n",
        "meant to be input into dataloader\n",
        "'''\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "class fhirJsonCustomDataset(Dataset):\n",
        "  def __init__(self, target_dir: str):\n",
        "    # Make a list of file paths\n",
        "    self.file_paths = [os.path.join(data_path, file_path) for file_path in os.listdir(target_dir)\n",
        "    if file_path.endswith('.json')]\n",
        "\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return(len(self.file_paths))\n",
        "\n",
        "  def __getitem__(self, index: int) -> torch.Tensor:\n",
        "\n",
        "    fhir_json_instance = SyntheaBundleModel(self.file_paths[index])\n",
        "    temp_input_tensor = create_model_input(fhir_json_instance.list_of_claim, MAX_CLAIM_C, CLAIM_ENCODE_SIZE)\n",
        "\n",
        "    return temp_input_tensor\n",
        "\n",
        "train_fhir_json_dataset = fhirJsonCustomDataset(target_dir=data_path) # make the dataset\n",
        "\n",
        "# len(train_fhir_json_dataset) # 108 individual files"
      ],
      "metadata": {
        "id": "CDJ6O4GvwLEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Dataloader\n",
        "'''\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader_custom = DataLoader(train_fhir_json_dataset, batch_size = BATCH_SIZE_C, num_workers=0, shuffle=True)\n",
        "\n",
        "test_input = next(iter(train_dataloader_custom)) # gets 1 batch, batchsize set to 8\n",
        "\n",
        "print(test_input.shape) # [8, 30, 24] this is a batch"
      ],
      "metadata": {
        "id": "zVl_DRpAbUhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "708d2a3d-c7d2-4fd5-c5db-8de87aa2ab63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded person claim count 19\n",
            "torch.Size([8, 30, 22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Training\n",
        "'''\n",
        "epochs = 3\n",
        "\n",
        "loss_fn = nn.MSELoss() # Mean sq err\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=.01)\n",
        "\n",
        "debug_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for batch, X in enumerate(train_dataloader_custom):\n",
        "    model_0.train()\n",
        "    reconstruction = model_0(X) # Get out the models reconstructed model, call forward once\n",
        "    loss = loss_fn(reconstruction, X)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    debug_list.append(f\"Epoch {epoch} Batch {batch} Loss: {loss.item()}\")\n",
        "\n",
        "for item in debug_list:\n",
        "  print(item)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qR5jlZwmYRR3",
        "outputId": "3a5f97cf-dc12-4707-80d1-0f716fb9a800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded person claim count 20\n",
            "Padded person claim count 28\n",
            "Padded person claim count 25\n",
            "Padded person claim count 23\n",
            "Padded person claim count 29\n",
            "Padded person claim count 25\n",
            "Padded person claim count 27\n",
            "Padded person claim count 23\n",
            "Padded person claim count 14\n",
            "Padded person claim count 21\n",
            "Padded person claim count 19\n",
            "Padded person claim count 12\n",
            "Padded person claim count 14\n",
            "Padded person claim count 16\n",
            "Padded person claim count 21\n",
            "Padded person claim count 8\n",
            "Padded person claim count 13\n",
            "Padded person claim count 17\n",
            "Padded person claim count 21\n",
            "Padded person claim count 25\n",
            "Padded person claim count 28\n",
            "Padded person claim count 12\n",
            "Padded person claim count 28\n",
            "Padded person claim count 25\n",
            "Padded person claim count 23\n",
            "Padded person claim count 25\n",
            "Padded person claim count 21\n",
            "Padded person claim count 27\n",
            "Padded person claim count 17\n",
            "Padded person claim count 16\n",
            "Padded person claim count 13\n",
            "Padded person claim count 14\n",
            "Padded person claim count 21\n",
            "Padded person claim count 23\n",
            "Padded person claim count 20\n",
            "Padded person claim count 29\n",
            "Padded person claim count 8\n",
            "Padded person claim count 19\n",
            "Padded person claim count 21\n",
            "Padded person claim count 14\n",
            "Padded person claim count 28\n",
            "Padded person claim count 25\n",
            "Padded person claim count 13\n",
            "Padded person claim count 21\n",
            "Padded person claim count 19\n",
            "Padded person claim count 21\n",
            "Padded person claim count 14\n",
            "Padded person claim count 12\n",
            "Padded person claim count 28\n",
            "Padded person claim count 17\n",
            "Padded person claim count 20\n",
            "Padded person claim count 16\n",
            "Padded person claim count 8\n",
            "Padded person claim count 14\n",
            "Padded person claim count 21\n",
            "Padded person claim count 23\n",
            "Padded person claim count 23\n",
            "Padded person claim count 27\n",
            "Padded person claim count 28\n",
            "Padded person claim count 29\n",
            "Padded person claim count 25\n",
            "Padded person claim count 25\n",
            "Padded person claim count 25\n",
            "Epoch 0 Batch 0 Loss: 5.3596343994140625\n",
            "Epoch 0 Batch 1 Loss: 5.620365619659424\n",
            "Epoch 0 Batch 2 Loss: 3.680046796798706\n",
            "Epoch 0 Batch 3 Loss: 3.4659013748168945\n",
            "Epoch 0 Batch 4 Loss: 7.139514446258545\n",
            "Epoch 0 Batch 5 Loss: 4.454771995544434\n",
            "Epoch 0 Batch 6 Loss: 3.5623228549957275\n",
            "Epoch 0 Batch 7 Loss: 4.242449760437012\n",
            "Epoch 0 Batch 8 Loss: 3.1704468727111816\n",
            "Epoch 0 Batch 9 Loss: 3.74914288520813\n",
            "Epoch 0 Batch 10 Loss: 7.972339630126953\n",
            "Epoch 0 Batch 11 Loss: 3.580888271331787\n",
            "Epoch 0 Batch 12 Loss: 5.06052303314209\n",
            "Epoch 0 Batch 13 Loss: 3.3772265911102295\n",
            "Epoch 1 Batch 0 Loss: 4.377349853515625\n",
            "Epoch 1 Batch 1 Loss: 4.429661750793457\n",
            "Epoch 1 Batch 2 Loss: 3.5666747093200684\n",
            "Epoch 1 Batch 3 Loss: 7.1901936531066895\n",
            "Epoch 1 Batch 4 Loss: 4.04576301574707\n",
            "Epoch 1 Batch 5 Loss: 5.171304702758789\n",
            "Epoch 1 Batch 6 Loss: 3.358560085296631\n",
            "Epoch 1 Batch 7 Loss: 8.191173553466797\n",
            "Epoch 1 Batch 8 Loss: 3.678175687789917\n",
            "Epoch 1 Batch 9 Loss: 4.061657905578613\n",
            "Epoch 1 Batch 10 Loss: 2.9415528774261475\n",
            "Epoch 1 Batch 11 Loss: 4.13710880279541\n",
            "Epoch 1 Batch 12 Loss: 5.30772066116333\n",
            "Epoch 1 Batch 13 Loss: 4.204099178314209\n",
            "Epoch 2 Batch 0 Loss: 3.9592602252960205\n",
            "Epoch 2 Batch 1 Loss: 3.8998923301696777\n",
            "Epoch 2 Batch 2 Loss: 4.749940395355225\n",
            "Epoch 2 Batch 3 Loss: 4.707612991333008\n",
            "Epoch 2 Batch 4 Loss: 4.543217182159424\n",
            "Epoch 2 Batch 5 Loss: 4.352142333984375\n",
            "Epoch 2 Batch 6 Loss: 5.652255058288574\n",
            "Epoch 2 Batch 7 Loss: 4.1193976402282715\n",
            "Epoch 2 Batch 8 Loss: 4.671354293823242\n",
            "Epoch 2 Batch 9 Loss: 4.967199325561523\n",
            "Epoch 2 Batch 10 Loss: 4.197781085968018\n",
            "Epoch 2 Batch 11 Loss: 3.6442160606384277\n",
            "Epoch 2 Batch 12 Loss: 3.1200814247131348\n",
            "Epoch 2 Batch 13 Loss: 18.354846954345703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Test Predicting\n",
        "'''\n",
        "\n",
        "fhir_json_instance_test = SyntheaBundleModel(file)\n",
        "\n",
        "tensor_test = create_model_input(fhir_json_instance.list_of_claim, MAX_CLAIM_C, CLAIM_ENCODE_SIZE)\n",
        "\n",
        "model_0.eval()\n",
        "reconstruction = model_0(tensor_test.unsqueeze(0))\n",
        "\n",
        "print(tensor_test.unsqueeze(0))\n",
        "print(reconstruction)\n",
        "\n",
        "error = torch.mean((reconstruction - tensor_test.unsqueeze(0)) ** 2, dim=(1, 2)) # - is element wise subtraction\n",
        "print(error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHe98-YwunYq",
        "outputId": "7fe13f47-ee69-4332-ff8b-5300184c1b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  2.,  2.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  3.,  3.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  2.,  2.,  1.,  1.],\n",
            "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  2.,  2.,  1.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  3.,  3.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1., 12., 12.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  3.,  3.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  2.,  2.,  1.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  3.,  3.,  1.,  1.],\n",
            "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1., 13., 13.,  1.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  3.,  3.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  2.,  2.,  1.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  7.,  7.,  1.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  4.,  4.,  0.,  1.],\n",
            "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1., 13., 13.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  2.,  2.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  5.,  5.,  1.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1., 12., 12.,  1.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  5.,  5.,  1.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1., 10., 10.,  0.,  1.],\n",
            "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  2.,  2.,  1.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  2.,  2.,  1.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  3.,  3.,  0.,  1.],\n",
            "         [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1., 15., 15.,  1.,  1.],\n",
            "         [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
            "           0.,  0.,  0.,  1.,  6.,  6.,  0.,  1.]]])\n",
            "tensor([[[ 0.0130, -0.0433, -0.0052,  0.0212,  0.0388, -0.0235,  0.0187,\n",
            "           0.0604, -0.0470, -0.0032, -0.0023,  0.0041, -0.0331,  0.0228,\n",
            "          -0.0638,  0.0352, -0.0020,  0.0412, -0.0106,  0.0479,  0.0244,\n",
            "           0.0828],\n",
            "         [ 0.0155, -0.0563, -0.0124,  0.0247,  0.0499, -0.0389,  0.0303,\n",
            "           0.0970, -0.0710, -0.0064, -0.0064,  0.0043, -0.0433,  0.0445,\n",
            "          -0.0925,  0.0483, -0.0042,  0.0617, -0.0044,  0.0824,  0.0283,\n",
            "           0.1248],\n",
            "         [ 0.0138, -0.0585, -0.0169,  0.0226,  0.0517, -0.0479,  0.0391,\n",
            "           0.1183, -0.0828, -0.0079, -0.0093,  0.0037, -0.0442,  0.0604,\n",
            "          -0.1050,  0.0505, -0.0056,  0.0697,  0.0039,  0.1041,  0.0249,\n",
            "           0.1473],\n",
            "         [ 0.0113, -0.0576, -0.0188,  0.0195,  0.0511, -0.0524,  0.0458,\n",
            "           0.1306, -0.0884, -0.0083, -0.0107,  0.0032, -0.0424,  0.0711,\n",
            "          -0.1100,  0.0483, -0.0062,  0.0717,  0.0107,  0.1167,  0.0201,\n",
            "           0.1598],\n",
            "         [ 0.0092, -0.0562, -0.0192,  0.0167,  0.0503, -0.0543,  0.0507,\n",
            "           0.1377, -0.0909, -0.0082, -0.0112,  0.0032, -0.0403,  0.0779,\n",
            "          -0.1119,  0.0449, -0.0063,  0.0714,  0.0153,  0.1237,  0.0159,\n",
            "           0.1669],\n",
            "         [ 0.0078, -0.0550, -0.0189,  0.0145,  0.0499, -0.0547,  0.0541,\n",
            "           0.1417, -0.0918, -0.0079, -0.0113,  0.0036, -0.0387,  0.0823,\n",
            "          -0.1124,  0.0417, -0.0062,  0.0705,  0.0183,  0.1273,  0.0128,\n",
            "           0.1708],\n",
            "         [ 0.0070, -0.0541, -0.0183,  0.0129,  0.0498, -0.0546,  0.0564,\n",
            "           0.1440, -0.0920, -0.0076, -0.0114,  0.0040, -0.0376,  0.0850,\n",
            "          -0.1125,  0.0391, -0.0060,  0.0698,  0.0201,  0.1292,  0.0108,\n",
            "           0.1729],\n",
            "         [ 0.0067, -0.0535, -0.0179,  0.0118,  0.0498, -0.0542,  0.0579,\n",
            "           0.1453, -0.0919, -0.0074, -0.0115,  0.0045, -0.0369,  0.0867,\n",
            "          -0.1124,  0.0373, -0.0058,  0.0693,  0.0213,  0.1302,  0.0095,\n",
            "           0.1740],\n",
            "         [ 0.0065, -0.0531, -0.0175,  0.0111,  0.0499, -0.0539,  0.0590,\n",
            "           0.1460, -0.0917, -0.0072, -0.0116,  0.0048, -0.0364,  0.0877,\n",
            "          -0.1123,  0.0361, -0.0056,  0.0691,  0.0219,  0.1306,  0.0088,\n",
            "           0.1745],\n",
            "         [ 0.0065, -0.0528, -0.0172,  0.0106,  0.0499, -0.0536,  0.0596,\n",
            "           0.1464, -0.0915, -0.0070, -0.0117,  0.0051, -0.0362,  0.0884,\n",
            "          -0.1122,  0.0352, -0.0055,  0.0689,  0.0224,  0.1308,  0.0083,\n",
            "           0.1748],\n",
            "         [ 0.0065, -0.0527, -0.0170,  0.0103,  0.0500, -0.0534,  0.0601,\n",
            "           0.1466, -0.0913, -0.0069, -0.0118,  0.0054, -0.0360,  0.0888,\n",
            "          -0.1121,  0.0347, -0.0054,  0.0689,  0.0226,  0.1309,  0.0081,\n",
            "           0.1749],\n",
            "         [ 0.0065, -0.0526, -0.0169,  0.0101,  0.0500, -0.0532,  0.0603,\n",
            "           0.1468, -0.0912, -0.0069, -0.0120,  0.0055, -0.0359,  0.0891,\n",
            "          -0.1120,  0.0344, -0.0054,  0.0689,  0.0228,  0.1309,  0.0080,\n",
            "           0.1749],\n",
            "         [ 0.0065, -0.0525, -0.0169,  0.0100,  0.0500, -0.0531,  0.0605,\n",
            "           0.1468, -0.0911, -0.0068, -0.0121,  0.0056, -0.0359,  0.0893,\n",
            "          -0.1120,  0.0341, -0.0053,  0.0689,  0.0229,  0.1309,  0.0079,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0525, -0.0169,  0.0099,  0.0500, -0.0531,  0.0606,\n",
            "           0.1468, -0.0910, -0.0068, -0.0122,  0.0057, -0.0358,  0.0894,\n",
            "          -0.1120,  0.0340, -0.0053,  0.0689,  0.0230,  0.1309,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0525, -0.0168,  0.0099,  0.0500, -0.0530,  0.0607,\n",
            "           0.1468, -0.0909, -0.0068, -0.0122,  0.0057, -0.0358,  0.0895,\n",
            "          -0.1120,  0.0339, -0.0053,  0.0689,  0.0230,  0.1309,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0099,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0909, -0.0067, -0.0123,  0.0057, -0.0358,  0.0895,\n",
            "          -0.1120,  0.0339, -0.0053,  0.0689,  0.0230,  0.1309,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0099,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0909, -0.0067, -0.0123,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1120,  0.0339, -0.0053,  0.0689,  0.0231,  0.1309,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0099,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0123,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1120,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749],\n",
            "         [ 0.0066, -0.0524, -0.0168,  0.0098,  0.0500, -0.0530,  0.0608,\n",
            "           0.1469, -0.0908, -0.0067, -0.0124,  0.0058, -0.0358,  0.0896,\n",
            "          -0.1119,  0.0338, -0.0053,  0.0689,  0.0231,  0.1308,  0.0078,\n",
            "           0.1749]]], grad_fn=<TransposeBackward0>)\n",
            "tensor([3.7652], grad_fn=<MeanBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "3/26/2025\n",
        "LSTM Autoenc seems to fail have not tweaked much but error goes very high and never decreases"
      ],
      "metadata": {
        "id": "WKWUEvXc8h-0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "185aeaad-49bc-4271-fddc-a882c2c9aa02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-47-d8085cecdefb>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-d8085cecdefb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    }
  ]
}